  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-02-01T22:25:39.550+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-02-01T22:25:39.551+0000[0m] {[34mexecutor_loader.py:[0m115} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-02-01 22:25:39 +0000] [30104] [INFO] Starting gunicorn 21.2.0
[2024-02-01 22:25:39 +0000] [30104] [ERROR] Connection in use: ('::', 8793)
[2024-02-01 22:25:39 +0000] [30104] [ERROR] Retrying in 1 second.
[[34m2024-02-01T22:25:39.571+0000[0m] {[34mscheduler_job_runner.py:[0m808} INFO[0m - Starting the scheduler[0m
[[34m2024-02-01T22:25:39.571+0000[0m] {[34mscheduler_job_runner.py:[0m815} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-02-01T22:25:39.575+0000[0m] {[34mmanager.py:[0m169} INFO[0m - Launched DagFileProcessorManager with pid: 30105[0m
[[34m2024-02-01T22:25:39.576+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:25:39.578+0000[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2024-02-01T22:25:39.587+0000[0m] {[34mscheduler_job_runner.py:[0m1642} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2024-02-01T22:25:39.592+0000] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-02-01 22:25:40 +0000] [30104] [ERROR] Connection in use: ('::', 8793)
[2024-02-01 22:25:40 +0000] [30104] [ERROR] Retrying in 1 second.
[2024-02-01 22:25:41 +0000] [30104] [ERROR] Connection in use: ('::', 8793)
[2024-02-01 22:25:41 +0000] [30104] [ERROR] Retrying in 1 second.
[2024-02-01 22:25:42 +0000] [30104] [ERROR] Connection in use: ('::', 8793)
[2024-02-01 22:25:42 +0000] [30104] [ERROR] Retrying in 1 second.
[2024-02-01 22:25:43 +0000] [30104] [ERROR] Connection in use: ('::', 8793)
[2024-02-01 22:25:43 +0000] [30104] [ERROR] Retrying in 1 second.
[2024-02-01 22:25:44 +0000] [30104] [ERROR] Can't connect to ('::', 8793)
[[34m2024-02-01T22:26:07.848+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:07.848+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T22:26:07.848+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:07.850+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task is_alpha_vantage_api_ready because previous state change time has not been saved[0m
[[34m2024-02-01T22:26:07.850+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-02-01T22:26:07.850+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:07.854+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:08.885+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T22:26:06.566008+00:00/task_id=is_alpha_vantage_api_ready permission to 509
[[34m2024-02-01T22:26:09.573+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-01T22:26:06.566008+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T22:26:12.942+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T22:26:12.949+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=is_alpha_vantage_api_ready, run_id=manual__2024-02-01T22:26:06.566008+00:00, map_index=-1, run_start_date=2024-02-01 22:26:09.607411+00:00, run_end_date=2024-02-01 22:26:12.499345+00:00, run_duration=2.891934, state=success, executor_state=success, try_number=1, max_tries=2, job_id=137, pool=default_pool, queue=default, priority_weight=5, operator=HttpSensor, queued_dttm=2024-02-01 22:26:07.849301+00:00, queued_by_job_id=136, pid=30130[0m
[[34m2024-02-01T22:26:13.203+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:13.203+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T22:26:13.203+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:13.204+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task fetch_stock_data because previous state change time has not been saved[0m
[[34m2024-02-01T22:26:13.204+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-02-01T22:26:13.205+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:13.208+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:14.213+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T22:26:06.566008+00:00/task_id=fetch_stock_data permission to 509
[[34m2024-02-01T22:26:14.974+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T22:26:06.566008+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T22:26:17.921+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T22:26:17.926+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=fetch_stock_data, run_id=manual__2024-02-01T22:26:06.566008+00:00, map_index=-1, run_start_date=2024-02-01 22:26:15.010555+00:00, run_end_date=2024-02-01 22:26:17.495866+00:00, run_duration=2.485311, state=success, executor_state=success, try_number=1, max_tries=2, job_id=138, pool=default_pool, queue=default, priority_weight=4, operator=SimpleHttpOperator, queued_dttm=2024-02-01 22:26:13.203900+00:00, queued_by_job_id=136, pid=30138[0m
[[34m2024-02-01T22:26:18.166+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:18.166+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T22:26:18.166+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:18.167+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task process_stock_data because previous state change time has not been saved[0m
[[34m2024-02-01T22:26:18.167+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-01T22:26:18.167+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:18.171+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:19.174+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T22:26:06.566008+00:00/task_id=process_stock_data permission to 509
[[34m2024-02-01T22:26:19.839+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T22:26:06.566008+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T22:26:20.812+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T22:26:20.817+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=process_stock_data, run_id=manual__2024-02-01T22:26:06.566008+00:00, map_index=-1, run_start_date=2024-02-01 22:26:19.870025+00:00, run_end_date=2024-02-01 22:26:20.420436+00:00, run_duration=0.550411, state=success, executor_state=success, try_number=1, max_tries=2, job_id=139, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-01 22:26:18.166904+00:00, queued_by_job_id=136, pid=30145[0m
[[34m2024-02-01T22:26:21.062+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:21.063+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T22:26:21.063+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:21.064+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task perform_descriptive_analysis because previous state change time has not been saved[0m
[[34m2024-02-01T22:26:21.064+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-01T22:26:21.064+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:21.068+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:22.090+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T22:26:06.566008+00:00/task_id=perform_descriptive_analysis permission to 509
[[34m2024-02-01T22:26:22.745+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-01T22:26:06.566008+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T22:26:23.712+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T22:26:23.716+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=perform_descriptive_analysis, run_id=manual__2024-02-01T22:26:06.566008+00:00, map_index=-1, run_start_date=2024-02-01 22:26:22.776352+00:00, run_end_date=2024-02-01 22:26:23.243218+00:00, run_duration=0.466866, state=success, executor_state=success, try_number=1, max_tries=2, job_id=140, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-01 22:26:21.063580+00:00, queued_by_job_id=136, pid=30152[0m
[[34m2024-02-01T22:26:23.751+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:23.751+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T22:26:23.751+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T22:26:06.566008+00:00 [scheduled]>[0m
[[34m2024-02-01T22:26:23.753+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task get_and_store_gpt_advice because previous state change time has not been saved[0m
[[34m2024-02-01T22:26:23.753+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-01T22:26:23.753+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:23.756+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-01T22:26:06.566008+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T22:26:24.777+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T22:26:06.566008+00:00/task_id=get_and_store_gpt_advice permission to 509
[[34m2024-02-01T22:26:25.417+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T22:26:06.566008+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T22:26:30.765+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-01T22:26:06.566008+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T22:26:30.769+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=get_and_store_gpt_advice, run_id=manual__2024-02-01T22:26:06.566008+00:00, map_index=-1, run_start_date=2024-02-01 22:26:25.450765+00:00, run_end_date=2024-02-01 22:26:30.360101+00:00, run_duration=4.909336, state=success, executor_state=success, try_number=1, max_tries=2, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-01 22:26:23.752177+00:00, queued_by_job_id=136, pid=30158[0m
[[34m2024-02-01T22:26:30.801+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun stock_analysis_dag @ 2024-02-01 22:26:06.566008+00:00: manual__2024-02-01T22:26:06.566008+00:00, state:running, queued_at: 2024-02-01 22:26:06.572716+00:00. externally triggered: True> successful[0m
[[34m2024-02-01T22:26:30.801+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=stock_analysis_dag, execution_date=2024-02-01 22:26:06.566008+00:00, run_id=manual__2024-02-01T22:26:06.566008+00:00, run_start_date=2024-02-01 22:26:07.820499+00:00, run_end_date=2024-02-01 22:26:30.801828+00:00, run_duration=22.981329, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-31 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=3a5e70ffe8c0bf89ae3d25ac55b3eae4[0m
[[34m2024-02-01T22:30:39.631+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:35:39.665+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:40:39.699+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:45:39.729+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:50:39.761+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T22:55:39.792+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:00:39.824+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:05:39.856+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:07:12.878+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:12.878+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T23:07:12.878+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:12.879+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task fetch_stock_data because previous state change time has not been saved[0m
[[34m2024-02-01T23:07:12.879+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-02-01T23:07:12.879+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:12.883+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:13.934+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T23:07:08.824599+00:00/task_id=fetch_stock_data permission to 509
[[34m2024-02-01T23:07:14.597+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-01T23:07:08.824599+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T23:07:15.801+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T23:07:15.805+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=fetch_stock_data, run_id=manual__2024-02-01T23:07:08.824599+00:00, map_index=-1, run_start_date=2024-02-01 23:07:14.629843+00:00, run_end_date=2024-02-01 23:07:15.342821+00:00, run_duration=0.712978, state=success, executor_state=success, try_number=1, max_tries=2, job_id=145, pool=default_pool, queue=default, priority_weight=4, operator=SimpleHttpOperator, queued_dttm=2024-02-01 23:07:12.878871+00:00, queued_by_job_id=136, pid=31215[0m
[[34m2024-02-01T23:07:15.843+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:15.843+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T23:07:15.844+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:15.845+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task process_stock_data because previous state change time has not been saved[0m
[[34m2024-02-01T23:07:15.846+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-01T23:07:15.846+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:15.850+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:16.863+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T23:07:08.824599+00:00/task_id=process_stock_data permission to 509
[[34m2024-02-01T23:07:17.547+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-01T23:07:08.824599+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T23:07:18.408+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T23:07:18.413+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=process_stock_data, run_id=manual__2024-02-01T23:07:08.824599+00:00, map_index=-1, run_start_date=2024-02-01 23:07:17.578772+00:00, run_end_date=2024-02-01 23:07:17.991125+00:00, run_duration=0.412353, state=success, executor_state=success, try_number=1, max_tries=2, job_id=146, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-01 23:07:15.844395+00:00, queued_by_job_id=136, pid=31221[0m
[[34m2024-02-01T23:07:20.536+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:20.536+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-01T23:07:20.536+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T23:07:08.824599+00:00 [scheduled]>[0m
[[34m2024-02-01T23:07:20.537+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task get_and_store_gpt_advice because previous state change time has not been saved[0m
[[34m2024-02-01T23:07:20.538+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-01T23:07:20.538+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:20.541+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-01T23:07:08.824599+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-01T23:07:21.577+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-01T23:07:08.824599+00:00/task_id=get_and_store_gpt_advice permission to 509
[[34m2024-02-01T23:07:22.279+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-01T23:07:08.824599+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-01T23:07:27.890+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-01T23:07:08.824599+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-01T23:07:27.894+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=get_and_store_gpt_advice, run_id=manual__2024-02-01T23:07:08.824599+00:00, map_index=-1, run_start_date=2024-02-01 23:07:22.320779+00:00, run_end_date=2024-02-01 23:07:27.528300+00:00, run_duration=5.207521, state=success, executor_state=success, try_number=1, max_tries=2, job_id=148, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-01 23:07:20.537105+00:00, queued_by_job_id=136, pid=31233[0m
[[34m2024-02-01T23:07:28.032+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun stock_analysis_dag @ 2024-02-01 23:07:08.824599+00:00: manual__2024-02-01T23:07:08.824599+00:00, state:running, queued_at: 2024-02-01 23:07:08.832535+00:00. externally triggered: True> successful[0m
[[34m2024-02-01T23:07:28.032+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=stock_analysis_dag, execution_date=2024-02-01 23:07:08.824599+00:00, run_id=manual__2024-02-01T23:07:08.824599+00:00, run_start_date=2024-02-01 23:07:09.526914+00:00, run_end_date=2024-02-01 23:07:28.032666+00:00, run_duration=18.505752, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-31 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=3a5e70ffe8c0bf89ae3d25ac55b3eae4[0m
[[34m2024-02-01T23:10:39.889+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:15:39.921+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:20:40.033+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:20:40.035+0000[0m] {[34mscheduler_job_runner.py:[0m1642} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-02-01T23:25:40.055+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:30:40.088+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:35:40.228+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:40:40.252+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:45:40.284+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:50:40.315+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-01T23:55:40.349+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:00:01.080+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for load_data_into_bigquery_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:00:01.086+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for stock_analysis_to_gcs_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:00:01.090+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for stock_analysis_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:00:01.125+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 3 tasks up for execution:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.is_api_available scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: load_data_into_bigquery_dag.load_data_to_bq scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:01.126+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:01.126+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_to_gcs_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:01.126+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG load_data_into_bigquery_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:01.126+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.is_api_available scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: load_data_into_bigquery_dag.load_data_to_bq scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task is_alpha_vantage_api_ready because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task is_api_available because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task load_data_to_bq because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:01.128+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='is_api_available', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-02T00:00:01.129+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'is_api_available', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:01.129+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='load_data_into_bigquery_dag', task_id='load_data_to_bq', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-02T00:00:01.129+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_data_into_bigquery_dag', 'load_data_to_bq', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_data_into_bigquery_dag.py'][0m
[[34m2024-02-02T00:00:01.132+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:02.189+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=is_alpha_vantage_api_ready permission to 509
[[34m2024-02-02T00:00:02.881+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:12.836+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'is_api_available', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:13.852+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_to_gcs_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_to_gcs_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=is_api_available permission to 509
[[34m2024-02-02T00:00:14.459+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_to_gcs_dag.is_api_available scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:16.269+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_data_into_bigquery_dag', 'load_data_to_bq', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_data_into_bigquery_dag.py'][0m
[[34m2024-02-02T00:00:17.277+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/load_data_into_bigquery_dag.py[0m
Changing /home/deprojectue/airflow_demo/logs/dag_id=load_data_into_bigquery_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=load_data_to_bq permission to 509
[[34m2024-02-02T00:00:17.912+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: load_data_into_bigquery_dag.load_data_to_bq scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:20.726+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:20.727+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='is_api_available', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:20.727+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_data_into_bigquery_dag', task_id='load_data_to_bq', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:20.733+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=is_alpha_vantage_api_ready, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:02.913075+00:00, run_end_date=2024-02-02 00:00:12.343262+00:00, run_duration=9.430187, state=success, executor_state=success, try_number=1, max_tries=2, job_id=149, pool=default_pool, queue=default, priority_weight=5, operator=HttpSensor, queued_dttm=2024-02-02 00:00:01.127051+00:00, queued_by_job_id=136, pid=31814[0m
[[34m2024-02-02T00:00:20.734+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=load_data_into_bigquery_dag, task_id=load_data_to_bq, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:17.941605+00:00, run_end_date=2024-02-02 00:00:20.337465+00:00, run_duration=2.39586, state=success, executor_state=success, try_number=1, max_tries=1, job_id=151, pool=default_pool, queue=default, priority_weight=2, operator=GCSToBigQueryOperator, queued_dttm=2024-02-02 00:00:01.127051+00:00, queued_by_job_id=136, pid=31826[0m
[[34m2024-02-02T00:00:20.734+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_to_gcs_dag, task_id=is_api_available, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:14.490983+00:00, run_end_date=2024-02-02 00:00:15.805405+00:00, run_duration=1.314422, state=success, executor_state=success, try_number=1, max_tries=1, job_id=150, pool=default_pool, queue=default, priority_weight=3, operator=HttpSensor, queued_dttm=2024-02-02 00:00:01.127051+00:00, queued_by_job_id=136, pid=31820[0m
[[34m2024-02-02T00:00:20.889+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 3 tasks up for execution:
	<TaskInstance: stock_analysis_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: load_data_into_bigquery_dag.transform_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:20.889+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:20.890+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_to_gcs_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:20.890+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG load_data_into_bigquery_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:20.890+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: load_data_into_bigquery_dag.transform_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:20.892+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task fetch_stock_data because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:20.892+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task fetch_stock_data because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:20.892+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task transform_data because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:20.893+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-02-02T00:00:20.893+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:20.893+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='fetch_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-02T00:00:20.893+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'fetch_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:20.893+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='load_data_into_bigquery_dag', task_id='transform_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-02T00:00:20.894+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_data_into_bigquery_dag', 'transform_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_data_into_bigquery_dag.py'][0m
[[34m2024-02-02T00:00:20.899+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:21.927+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=fetch_stock_data permission to 509
[[34m2024-02-02T00:00:22.592+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:24.424+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'fetch_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:25.442+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_to_gcs_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_to_gcs_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=fetch_stock_data permission to 509
[[34m2024-02-02T00:00:26.055+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_to_gcs_dag.fetch_stock_data scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:27.599+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_data_into_bigquery_dag', 'transform_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_data_into_bigquery_dag.py'][0m
[[34m2024-02-02T00:00:28.634+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/load_data_into_bigquery_dag.py[0m
Changing /home/deprojectue/airflow_demo/logs/dag_id=load_data_into_bigquery_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=transform_data permission to 509
[[34m2024-02-02T00:00:29.345+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: load_data_into_bigquery_dag.transform_data scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:29.877+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:29.878+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='fetch_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:29.878+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_data_into_bigquery_dag', task_id='transform_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:29.882+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=fetch_stock_data, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:22.630066+00:00, run_end_date=2024-02-02 00:00:24.052699+00:00, run_duration=1.422633, state=success, executor_state=success, try_number=1, max_tries=2, job_id=152, pool=default_pool, queue=default, priority_weight=4, operator=SimpleHttpOperator, queued_dttm=2024-02-02 00:00:20.891139+00:00, queued_by_job_id=136, pid=31833[0m
[[34m2024-02-02T00:00:29.882+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=load_data_into_bigquery_dag, task_id=transform_data, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:29.374118+00:00, run_end_date=2024-02-02 00:00:29.458542+00:00, run_duration=0.084424, state=success, executor_state=success, try_number=1, max_tries=1, job_id=154, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-02 00:00:20.891139+00:00, queued_by_job_id=136, pid=31861[0m
[[34m2024-02-02T00:00:29.883+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_to_gcs_dag, task_id=fetch_stock_data, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:26.090838+00:00, run_end_date=2024-02-02 00:00:27.167032+00:00, run_duration=1.076194, state=success, executor_state=success, try_number=1, max_tries=1, job_id=153, pool=default_pool, queue=default, priority_weight=2, operator=SimpleHttpOperator, queued_dttm=2024-02-02 00:00:20.891139+00:00, queued_by_job_id=136, pid=31839[0m
[[34m2024-02-02T00:00:30.020+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun load_data_into_bigquery_dag @ 2024-02-01 00:00:00+00:00: scheduled__2024-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-02 00:00:01.072478+00:00. externally triggered: False> successful[0m
[[34m2024-02-02T00:00:30.021+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=load_data_into_bigquery_dag, execution_date=2024-02-01 00:00:00+00:00, run_id=scheduled__2024-02-01T00:00:00+00:00, run_start_date=2024-02-02 00:00:01.100063+00:00, run_end_date=2024-02-02 00:00:30.021003+00:00, run_duration=28.92094, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-02-01 00:00:00+00:00, data_interval_end=2024-02-02 00:00:00+00:00, dag_hash=d1eaf8ec408c21a962a32c24463730bc[0m
[[34m2024-02-02T00:00:30.024+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for load_data_into_bigquery_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:00:30.038+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 2 tasks up for execution:
	<TaskInstance: stock_analysis_dag.process_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.process_and_save_data_to_gcs scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:30.038+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:30.038+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_to_gcs_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:30.039+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.process_stock_data scheduled__2024-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: stock_analysis_to_gcs_dag.process_and_save_data_to_gcs scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:30.040+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task process_stock_data because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:30.040+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task process_and_save_data_to_gcs because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:30.041+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-02T00:00:30.041+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:30.041+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='process_and_save_data_to_gcs', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-02T00:00:30.041+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'process_and_save_data_to_gcs', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:30.045+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:31.063+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=process_stock_data permission to 509
[[34m2024-02-02T00:00:31.744+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.process_stock_data scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:32.583+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_to_gcs_dag', 'process_and_save_data_to_gcs', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_to_gcs_dag.py'][0m
[[34m2024-02-02T00:00:33.617+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_to_gcs_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_to_gcs_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=process_and_save_data_to_gcs permission to 509
[[34m2024-02-02T00:00:34.219+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_to_gcs_dag.process_and_save_data_to_gcs scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:34.963+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:34.963+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_to_gcs_dag', task_id='process_and_save_data_to_gcs', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:34.970+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=process_stock_data, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:31.778023+00:00, run_end_date=2024-02-02 00:00:32.137231+00:00, run_duration=0.359208, state=success, executor_state=success, try_number=1, max_tries=2, job_id=155, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-02 00:00:30.039510+00:00, queued_by_job_id=136, pid=31868[0m
[[34m2024-02-02T00:00:34.971+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_to_gcs_dag, task_id=process_and_save_data_to_gcs, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:34.250349+00:00, run_end_date=2024-02-02 00:00:34.604180+00:00, run_duration=0.353831, state=success, executor_state=success, try_number=1, max_tries=1, job_id=156, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-02 00:00:30.039510+00:00, queued_by_job_id=136, pid=31874[0m
[[34m2024-02-02T00:00:35.112+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun stock_analysis_to_gcs_dag @ 2024-02-01 00:00:00+00:00: scheduled__2024-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-02 00:00:01.083077+00:00. externally triggered: False> successful[0m
[[34m2024-02-02T00:00:35.112+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=stock_analysis_to_gcs_dag, execution_date=2024-02-01 00:00:00+00:00, run_id=scheduled__2024-02-01T00:00:00+00:00, run_start_date=2024-02-02 00:00:01.100612+00:00, run_end_date=2024-02-02 00:00:35.112902+00:00, run_duration=34.01229, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-02-01 00:00:00+00:00, data_interval_end=2024-02-02 00:00:00+00:00, dag_hash=450cb444be7a9d2eb6553b4c1e9cdb94[0m
[[34m2024-02-02T00:00:35.116+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for stock_analysis_to_gcs_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:00:35.125+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:35.125+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:35.125+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:35.126+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task perform_descriptive_analysis because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:35.126+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-02T00:00:35.127+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:35.130+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:36.156+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=perform_descriptive_analysis permission to 509
[[34m2024-02-02T00:00:36.796+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.perform_descriptive_analysis scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:37.588+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:37.592+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=perform_descriptive_analysis, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:36.827264+00:00, run_end_date=2024-02-02 00:00:37.141524+00:00, run_duration=0.31426, state=success, executor_state=success, try_number=1, max_tries=2, job_id=157, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-02 00:00:35.125855+00:00, queued_by_job_id=136, pid=31881[0m
[[34m2024-02-02T00:00:37.626+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:37.626+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T00:00:37.626+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice scheduled__2024-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-02T00:00:37.627+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task get_and_store_gpt_advice because previous state change time has not been saved[0m
[[34m2024-02-02T00:00:37.628+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-02T00:00:37.628+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:37.631+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'scheduled__2024-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T00:00:38.676+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=scheduled__2024-02-01T00:00:00+00:00/task_id=get_and_store_gpt_advice permission to 509
[[34m2024-02-02T00:00:39.316+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.get_and_store_gpt_advice scheduled__2024-02-01T00:00:00+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T00:00:41.890+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='scheduled__2024-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T00:00:41.894+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=get_and_store_gpt_advice, run_id=scheduled__2024-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-02 00:00:39.347776+00:00, run_end_date=2024-02-02 00:00:41.505376+00:00, run_duration=2.1576, state=success, executor_state=success, try_number=1, max_tries=2, job_id=158, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-02 00:00:37.627308+00:00, queued_by_job_id=136, pid=31887[0m
[[34m2024-02-02T00:00:41.912+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:00:41.933+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun stock_analysis_dag @ 2024-02-01 00:00:00+00:00: scheduled__2024-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-02 00:00:01.087781+00:00. externally triggered: False> successful[0m
[[34m2024-02-02T00:00:41.933+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=stock_analysis_dag, execution_date=2024-02-01 00:00:00+00:00, run_id=scheduled__2024-02-01T00:00:00+00:00, run_start_date=2024-02-02 00:00:01.100528+00:00, run_end_date=2024-02-02 00:00:41.933723+00:00, run_duration=40.833195, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-02-01 00:00:00+00:00, data_interval_end=2024-02-02 00:00:00+00:00, dag_hash=3a5e70ffe8c0bf89ae3d25ac55b3eae4[0m
[[34m2024-02-02T00:00:41.936+0000[0m] {[34mdag.py:[0m3820} INFO[0m - Setting next_dagrun for stock_analysis_dag to 2024-02-02T00:00:00+00:00, run_after=2024-02-03T00:00:00+00:00[0m
[[34m2024-02-02T00:05:41.936+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:10:41.960+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:15:41.985+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:20:42.011+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:25:42.037+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:30:42.065+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:35:42.091+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:40:42.116+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:45:42.141+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:50:42.167+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T00:55:42.195+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:00:42.220+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:05:42.245+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:10:42.270+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:15:42.295+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:20:42.321+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:25:42.346+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:30:42.373+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:35:42.398+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:40:42.422+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:45:42.447+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:50:42.473+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T01:55:42.497+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:00:42.524+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:05:42.547+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:10:42.570+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:15:42.593+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:20:42.618+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:25:42.641+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:30:42.667+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:35:42.692+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:40:42.716+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:45:42.740+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:50:42.765+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T02:55:42.790+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:00:42.817+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:05:42.843+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:10:42.867+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:15:42.890+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:20:42.914+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:25:42.939+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:30:42.965+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:35:42.989+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:40:43.014+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:45:43.039+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:50:43.063+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T03:55:43.090+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:00:43.117+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:05:43.142+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:10:43.166+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:15:43.191+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:20:43.218+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:25:43.244+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:30:43.270+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:35:43.295+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:40:43.320+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:45:43.345+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:50:43.369+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T04:55:43.395+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:00:43.455+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:05:43.481+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:10:43.508+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:15:43.533+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:20:43.558+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:25:43.576+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:30:43.600+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:35:43.627+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:40:43.650+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:45:43.675+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:50:43.700+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T05:55:43.727+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:00:43.752+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:05:43.777+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:10:43.802+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:15:43.825+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:20:43.850+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:25:43.877+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:30:43.901+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:35:43.927+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:40:43.950+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:45:43.975+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:50:44.001+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T06:55:44.026+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:00:44.051+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:05:44.077+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:10:44.101+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:15:44.125+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:20:44.150+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:25:44.175+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:30:44.199+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:35:44.224+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:40:44.247+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:45:44.273+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:50:44.299+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T07:55:44.324+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:00:44.351+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:05:44.377+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:10:44.402+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:15:44.427+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:20:44.454+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:25:44.480+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:30:44.504+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:35:44.530+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:40:44.556+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:45:44.580+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:50:44.606+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T08:55:44.632+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:00:44.658+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:05:44.683+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:10:44.708+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:15:44.733+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:20:44.760+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:25:44.785+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:30:44.804+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:35:44.829+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:40:44.856+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:45:44.881+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:50:44.906+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T09:55:44.930+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:00:44.957+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:05:44.982+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:10:45.008+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:15:45.033+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:20:45.057+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:25:45.083+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:30:45.106+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:35:45.131+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:40:45.155+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:45:45.180+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:50:45.187+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T10:55:45.212+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:00:45.344+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:05:45.371+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:10:45.399+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:15:45.424+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:20:45.451+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:25:45.467+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:30:45.492+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:35:45.518+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:40:45.541+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:45:45.566+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:50:45.591+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T11:55:45.616+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:00:45.641+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:05:45.665+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:10:45.688+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:15:45.713+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:20:45.740+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:25:45.772+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:30:45.796+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:35:45.821+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:40:45.847+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:45:45.872+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:50:45.890+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T12:55:45.914+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:00:45.939+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:05:45.963+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:10:45.988+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:15:46.007+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:20:46.031+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:25:46.056+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:30:46.128+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:35:46.156+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:40:46.179+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:45:46.205+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-02T13:50:24.032+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:24.032+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T13:50:24.033+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:24.033+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task is_alpha_vantage_api_ready because previous state change time has not been saved[0m
[[34m2024-02-02T13:50:24.034+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2024-02-02T13:50:24.038+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:24.042+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'is_alpha_vantage_api_ready', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:25.095+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-02T13:50:23.445765+00:00/task_id=is_alpha_vantage_api_ready permission to 509
[[34m2024-02-02T13:50:25.752+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.is_alpha_vantage_api_ready manual__2024-02-02T13:50:23.445765+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T13:50:27.057+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='is_alpha_vantage_api_ready', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T13:50:27.061+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=is_alpha_vantage_api_ready, run_id=manual__2024-02-02T13:50:23.445765+00:00, map_index=-1, run_start_date=2024-02-02 13:50:25.783363+00:00, run_end_date=2024-02-02 13:50:26.670211+00:00, run_duration=0.886848, state=success, executor_state=success, try_number=1, max_tries=2, job_id=159, pool=default_pool, queue=default, priority_weight=5, operator=HttpSensor, queued_dttm=2024-02-02 13:50:24.033365+00:00, queued_by_job_id=136, pid=50014[0m
[[34m2024-02-02T13:50:27.106+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:27.106+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T13:50:27.106+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:27.107+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task fetch_stock_data because previous state change time has not been saved[0m
[[34m2024-02-02T13:50:27.108+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2024-02-02T13:50:27.108+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:27.111+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'fetch_stock_data', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:28.118+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-02T13:50:23.445765+00:00/task_id=fetch_stock_data permission to 509
[[34m2024-02-02T13:50:28.774+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.fetch_stock_data manual__2024-02-02T13:50:23.445765+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T13:50:29.889+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='fetch_stock_data', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T13:50:29.893+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=fetch_stock_data, run_id=manual__2024-02-02T13:50:23.445765+00:00, map_index=-1, run_start_date=2024-02-02 13:50:28.807240+00:00, run_end_date=2024-02-02 13:50:29.467962+00:00, run_duration=0.660722, state=success, executor_state=success, try_number=1, max_tries=2, job_id=160, pool=default_pool, queue=default, priority_weight=4, operator=SimpleHttpOperator, queued_dttm=2024-02-02 13:50:27.107052+00:00, queued_by_job_id=136, pid=50021[0m
[[34m2024-02-02T13:50:30.032+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:30.032+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T13:50:30.032+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:30.033+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task process_stock_data because previous state change time has not been saved[0m
[[34m2024-02-02T13:50:30.034+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-02T13:50:30.034+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:30.037+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'process_stock_data', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:31.052+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-02T13:50:23.445765+00:00/task_id=process_stock_data permission to 509
[[34m2024-02-02T13:50:31.706+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.process_stock_data manual__2024-02-02T13:50:23.445765+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T13:50:32.597+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='process_stock_data', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T13:50:32.601+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=process_stock_data, run_id=manual__2024-02-02T13:50:23.445765+00:00, map_index=-1, run_start_date=2024-02-02 13:50:31.737357+00:00, run_end_date=2024-02-02 13:50:32.216914+00:00, run_duration=0.479557, state=success, executor_state=success, try_number=1, max_tries=2, job_id=161, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-02 13:50:30.033148+00:00, queued_by_job_id=136, pid=50028[0m
[[34m2024-02-02T13:50:32.746+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:32.747+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T13:50:32.747+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:32.748+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task perform_descriptive_analysis because previous state change time has not been saved[0m
[[34m2024-02-02T13:50:32.748+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-02T13:50:32.748+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:32.752+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'perform_descriptive_analysis', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:33.741+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-02T13:50:23.445765+00:00/task_id=perform_descriptive_analysis permission to 509
[[34m2024-02-02T13:50:34.427+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.perform_descriptive_analysis manual__2024-02-02T13:50:23.445765+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T13:50:35.248+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='perform_descriptive_analysis', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T13:50:35.252+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=perform_descriptive_analysis, run_id=manual__2024-02-02T13:50:23.445765+00:00, map_index=-1, run_start_date=2024-02-02 13:50:34.460981+00:00, run_end_date=2024-02-02 13:50:34.870810+00:00, run_duration=0.409829, state=success, executor_state=success, try_number=1, max_tries=2, job_id=162, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-02 13:50:32.747529+00:00, queued_by_job_id=136, pid=50035[0m
[[34m2024-02-02T13:50:35.392+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:35.393+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG stock_analysis_dag has 0/16 running and queued tasks[0m
[[34m2024-02-02T13:50:35.393+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-02T13:50:23.445765+00:00 [scheduled]>[0m
[[34m2024-02-02T13:50:35.394+0000[0m] {[34mtaskinstance.py:[0m2261} WARNING[0m - cannot record scheduled_duration for task get_and_store_gpt_advice because previous state change time has not been saved[0m
[[34m2024-02-02T13:50:35.394+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-02T13:50:35.394+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:35.398+0000[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'stock_analysis_dag', 'get_and_store_gpt_advice', 'manual__2024-02-02T13:50:23.445765+00:00', '--local', '--subdir', 'DAGS_FOLDER/stock_analysis_dag.py'][0m
[[34m2024-02-02T13:50:36.400+0000[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /home/deprojectue/airflow_demo/dags/stock_analysis_dag.py[0m
/home/deprojectue/miniconda3/envs/airflow_demo/lib/python3.9/site-packages/airflow/models/baseoperator.py:437 AirflowProviderDeprecationWarning: Class `SimpleHttpOperator` is deprecated and will be removed in a future release. Please use `HttpOperator` instead.
Changing /home/deprojectue/airflow_demo/logs/dag_id=stock_analysis_dag/run_id=manual__2024-02-02T13:50:23.445765+00:00/task_id=get_and_store_gpt_advice permission to 509
[[34m2024-02-02T13:50:37.057+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: stock_analysis_dag.get_and_store_gpt_advice manual__2024-02-02T13:50:23.445765+00:00 [queued]> on host de-airflow-vm-instance-1.europe-west10-a.c.dataengineeringfinal-411911.internal[0m
[[34m2024-02-02T13:50:41.332+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='stock_analysis_dag', task_id='get_and_store_gpt_advice', run_id='manual__2024-02-02T13:50:23.445765+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-02T13:50:41.336+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=stock_analysis_dag, task_id=get_and_store_gpt_advice, run_id=manual__2024-02-02T13:50:23.445765+00:00, map_index=-1, run_start_date=2024-02-02 13:50:37.101542+00:00, run_end_date=2024-02-02 13:50:40.848615+00:00, run_duration=3.747073, state=success, executor_state=success, try_number=1, max_tries=2, job_id=163, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-02 13:50:35.393581+00:00, queued_by_job_id=136, pid=50042[0m
[[34m2024-02-02T13:50:41.368+0000[0m] {[34mdagrun.py:[0m732} INFO[0m - Marking run <DagRun stock_analysis_dag @ 2024-02-02 13:50:23.445765+00:00: manual__2024-02-02T13:50:23.445765+00:00, state:running, queued_at: 2024-02-02 13:50:23.455918+00:00. externally triggered: True> successful[0m
[[34m2024-02-02T13:50:41.368+0000[0m] {[34mdagrun.py:[0m783} INFO[0m - DagRun Finished: dag_id=stock_analysis_dag, execution_date=2024-02-02 13:50:23.445765+00:00, run_id=manual__2024-02-02T13:50:23.445765+00:00, run_start_date=2024-02-02 13:50:24.016143+00:00, run_end_date=2024-02-02 13:50:41.368918+00:00, run_duration=17.352775, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-02-01 00:00:00+00:00, data_interval_end=2024-02-02 00:00:00+00:00, dag_hash=3a5e70ffe8c0bf89ae3d25ac55b3eae4[0m
[[34m2024-02-02T13:50:46.231+0000[0m] {[34mscheduler_job_runner.py:[0m1619} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
